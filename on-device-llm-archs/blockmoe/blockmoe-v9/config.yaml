main_dim: 768
ffn_factor: 2.5
main_attn_num: 12
moe_layer_num: 8
first_k_layer_dense: 2
last_k_layer_dense: 2
aux_dim: 256
aux_attn_num: 8
vocab_size: 50304
eot_idx: 50256
token_keys: true
token_router_bias: true
shared_expert: true
shared_expert_dim: 1920
pf_shared_expert: false
p_global_mask: false

p_block_size: 16
p_aux_layer_num: 6
p_routed_expert_num: 64
p_expert_num_per_token: 4
p_router_group_num: 1
p_balance_loss_factor: 0.01
p_main_balance_loss_coeff: 0.0
p_expert_dim: 512
p_routed_scaling_factor: 2.0

f_block_size: 16
f_aux_layer_num: 6
f_routed_expert_num: 32
f_expert_num_per_token: 2
f_router_group_num: 1
f_balance_loss_factor: 0.01
f_main_balance_loss_coeff: 0.1
f_expert_dim: 128
f_routed_scaling_factor: 0.5

seq_len: 2048
init_std: 0.02
max_lr: 3.0e-4
aux_max_lr: 1.5e-4
theta: 10000.0
theta_s: 1000.0
weight_decay: 0.1
adam_betas: [0.9, 0.95]
grad_clip: 1.0
norm_eps: 1.0e-8
adam_eps: 1.0e-8
dtype: "bfloat16"
tied_vocab_emb: true
flex_attn_spec_kernel: true
zero_optimizer: false
grouped_gemm_option: "for_loop"
grouped_gemm_func: false
grouped_gemm_checkpoint: false
kernel_options:
  BLOCK_M: 64
  BLOCK_N: 64
  BLOCK_M1: 32
  BLOCK_N1: 64
  BLOCK_M2: 64
  BLOCK_N2: 32

train_bin: "/root/autodl-tmp/llm.c/dev/data/fineweb10B/fineweb_train_*.bin"
val_bin: "/root/autodl-tmp/llm.c/dev/data/fineweb10B/fineweb_val_*.bin"
aux_batch_size: 16
aux_sum_steps: 4
batch_size: 4
sum_steps: 16
num_steps: 10000
warmup_steps: 100
cooldown_steps: 1000
val_every: 50
val_steps: 20
min_lr_ratio: 0.01
save: false
cooldown_save: true
overfit: false
mixed_precision: true
scaler: true
model_compile: false
wandb_mode: "disabled"
load_from_checkpoint: false
checkpoint_path: " "
load_model: false
load_data: false
load_optimizer: false
profiler: false
profiler_step: 5
memory_profiler: false
memory_profiler_start_step: 0
memory_profiler_end_step: 2