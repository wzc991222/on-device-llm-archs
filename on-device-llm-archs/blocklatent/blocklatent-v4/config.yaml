train_mode: "pre"
latent_loss_type: "cross_entropy"
main_mode_init: false
encoder_mask: true
main_encoder_mask: true
mask_rate: 0.15
latent_vq: true
vq_rate: 0.3
vq_st: false
unmask_factor: 0.05
sole_latent_vocab: false
copy_latent_vocab: false
latent_loss: true
codebook_loss: true
orthog_loss: false
latent_loss_factor: 0.5
codebook_loss_type: "cross_entropy"
codebook_loss_factor: 0.1
codebook_threshold: 0.5
codebook_label_prob_mode: true
codebook_label_prob_norm: false
aux_loss_factor: 0.05
orthog_loss_factor: 0.0

main_dim: 512
main_attn_num: 8
ffn_factor: 2.5
ar_decoder_dim: 512
ar_decoder_attn_num: 8

block_size: 16
latent_size: 16
decoder_size: 32
codebook_size: 12800
vocab_size: 50304
eot_idx: 50256
pad_idx: 50257
mask_idx: 50258

encoder_layer_num: 4
aux_decoder_layer_num: 2
main_layer_num: 10
decoder_layer_num: 2
parallel_layer_num: 2
ar_decoder_layer_num: 2

seq_len: 2048
init_std: 0.02
max_lr: 3.0e-4
theta: 10000.0
theta_s: 500.0
weight_decay: 0.1
adam_betas: [0.9, 0.95]
grad_clip: 1.0
norm_eps: 1.0e-6
adam_eps: 1.0e-8
dtype: "bfloat16"
pre_tied_vocab_emb: true
tied_vocab_emb: true
qk_norm: false
flex_attn_spec_kernel: true
zero_optimizer: false
kernel_options:
  BLOCK_M: 64
  BLOCK_N: 64
  BLOCK_M1: 32
  BLOCK_N1: 64
  BLOCK_M2: 64
  BLOCK_N2: 32

encoder_inplace: false
encoder_global_attn: true
encoder_old_full_attn: false
encoder_new_full_attn: false
encoder_token_pos_emb: false
aux_decoder_global_attn: false
aux_decoder_old_full_attn: false
aux_decoder_new_full_attn: true
aux_decoder_token_pos_emb: false
main_token_pos_emb: true
main_type_1_attn: false
main_type_2_attn: true
decoder_inplace: false
decoder_global_attn: true
decoder_old_full_attn: false
decoder_new_full_attn: true
decoder_token_pos_emb: false

parallel_init: false
parallel_mode: false
pre_parallel_num: 2
post_parallel_num: 4
ar_old_norm: false
ar_new_norm: true
ar_decoder_input: true
ar_decoder_sole_input: false
iteration_init: false
iteration_mode: true
ar_iteration_num: 2
ar_attn_window_len: 16

lm_head_c_init: false
lm_head_c_mode: false
lm_head_c_num: 16
lm_head_vec_num: 8
y_vec_loss_factor: 0.05
vec_label_loss_factor: 0.01
lm_head_aux_loss_factor: 0.05
model_grad_detach: false
vec_topk_num: 4
y_label_vec_loss: true

train_bin: "/root/autodl-tmp/llm.c/dev/data/fineweb10B/fineweb_train_*.bin"
val_bin: "/root/autodl-tmp/llm.c/dev/data/fineweb10B/fineweb_val_*.bin"
batch_size: 8
accumulated_steps: 1
num_steps: 4000
warmup_steps: 100
cooldown_steps: 500
val_every: 50
val_steps: 20
lr_scheduler: "wsd"  # "wsd", "cos"
min_lr_ratio: 0.01
save: true
beginning_save_step: 10
save_every: 2000
save_before_cooldown: true
keep_latest_k: 2
overfit: false
mixed_precision: true
scaler: true
model_compile: false
wandb_mode: "disabled"  # "online", "offline", "disabled"
load_from_checkpoint: false
checkpoint_path: " "
load_model: true
load_data: false
load_optimizer: false
profiler: false
profiler_step: 5
memory_profiler: false
memory_profiler_start_step: 6
memory_profiler_end_step: 8