{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e8c1d6-83ec-4381-b92f-daaeec4a43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f24f386-8181-4032-b398-c4b8633ec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangzongcheng/wzc/code/latent-v4/model.py:9: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d7f6ef-f3b5-489c-bbeb-6469d97255b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc507d68-5b2e-4833-9826-b52c64a8ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"./config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bff694e-8a42-48b7-a709-b51bcde97a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0847c2db-ce32-451f-897e-29e8083db231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'encoder_params': 12070912,\n",
       "  'aux_decocder_params': 6039552,\n",
       "  'main_params': 30164992,\n",
       "  'decoder_params': 6047744,\n",
       "  'parallel_params': 7604224,\n",
       "  'ar_decoder_params': 9178112,\n",
       "  'model_params': 71105536,\n",
       "  'vocab_params': 64685568,\n",
       "  'total_params': 135791104},\n",
       " {'encoder_a_params': 24141824,\n",
       "  'aux_decoder_a_params': 12079104,\n",
       "  'main_a_params': 30164992,\n",
       "  'decoder_a_params': 18143232,\n",
       "  'parallel_a_params': 25174016,\n",
       "  'ar_decoder_a_params': 74474496,\n",
       "  'model_a_params': 208319488,\n",
       "  'vocab_a_params': 71172096,\n",
       "  'total_a_params': 279491584,\n",
       "  'infer_model_a_params': 172098560,\n",
       "  'infer_vocab_a_params': 25755648,\n",
       "  'infer_total_a_params': 197854208})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params_count(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc705543-2470-4ad7-bddf-d02f37d5ce3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (vocab_emb): Embedding(50304, 512)\n",
       "  (ar_vocab_emb): Embedding(50304, 512)\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (aux_decoder): AuxDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (main_model): MainModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-9): 10 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (parallel): Parallel(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pre_parallel_linear): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (post_parallel_linear): Linear(in_features=512, out_features=2048, bias=False)\n",
       "    (parallel_linear): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (ar_decoder): ARDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Layer(\n",
       "        (attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ffn_up): Linear(in_features=512, out_features=2560, bias=False)\n",
       "        (ffn_down): Linear(in_features=1280, out_features=512, bias=False)\n",
       "        (attn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ar_new_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (x_old_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (x_ar_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (x_ar_linear_list): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "    (x_de_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (x_de_linear_list): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "    (y_linear_list): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "    (pre_agg_linear): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (post_agg_linear): Linear(in_features=2048, out_features=512, bias=False)\n",
       "    (agg_linear): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (attention): Attention(\n",
       "      (rotary_emb): RotaryEmb()\n",
       "    )\n",
       "  )\n",
       "  (pre_lm_head): Linear(in_features=512, out_features=50304, bias=False)\n",
       "  (lm_head): LMHead(\n",
       "    (lm_head): Linear(in_features=512, out_features=50304, bias=False)\n",
       "  )\n",
       "  (y_latent_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (label_latent_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (final_norm): RMSNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d728d12-dc2a-43e8-9d9e-16f951f02722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "223c4d37-40ad-402d-8e88-b623a60dd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eot_idx = 1\n",
    "pad_idx = 0\n",
    "b = 8\n",
    "s = 64\n",
    "block_size = 8\n",
    "buffer = torch.randint(1, 16, (512,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6140c419-9d65-4e61-aadc-7a61daa893d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mfull((b, s \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), pad_idx, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      2\u001b[0m eot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margwhere(buffer \u001b[38;5;241m==\u001b[39m eot_idx)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m eot_num \u001b[38;5;241m=\u001b[39m eot\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = torch.full((b, s + 1), pad_idx, dtype=torch.long)\n",
    "eot = torch.argwhere(buffer == eot_idx).squeeze(-1)\n",
    "eot_num = eot.shape[0]\n",
    "b_pos_left, b_pos_right, eot_pos = 0, 0, 0\n",
    "\n",
    "for row in range(b):\n",
    "    pos = 1\n",
    "    while pos <= s:\n",
    "        pad_num = random.randint(0, block_size - 1)\n",
    "        pos += pad_num\n",
    "        rest_len = s + 1 - pos\n",
    "\n",
    "        if eot_pos >= eot_num or (\n",
    "            eot_pos < eot_num and eot[eot_pos] - b_pos_left + 1 > rest_len\n",
    "        ):\n",
    "            b_pos_right = b_pos_left + rest_len\n",
    "            tokens[row, pos:] = buffer[b_pos_left:b_pos_right]\n",
    "            pos = s + 1\n",
    "            b_pos_left = b_pos_right\n",
    "        else:\n",
    "            b_pos_right = eot[eot_pos] + 1\n",
    "            s_len = b_pos_right - b_pos_left\n",
    "            tokens[row, pos : pos + s_len] = buffer[b_pos_left:b_pos_right]\n",
    "            end_pad = (block_size - (pos + s_len - 1) % block_size) % block_size\n",
    "            pos = pos + s_len + end_pad\n",
    "            b_pos_left = b_pos_right\n",
    "            eot_pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46887a33-a0b8-48a5-9fc7-c7a94deaf8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 10,  3,  4,  7, 15, 11,  1],\n",
       "        [ 0,  0,  0,  0,  0,  9,  4,  2],\n",
       "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 14,  8,  5,  7, 13],\n",
       "        [12, 14, 14, 10,  9, 10,  9, 12],\n",
       "        [ 9,  8,  6, 12, 15,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  5,  5,  2],\n",
       "        [ 4,  1,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  4,  2,  5,  4, 10, 15, 10],\n",
       "        [11,  8,  8, 10,  7,  2, 11, 12],\n",
       "        [ 9, 10,  5, 10,  2,  3, 11, 14],\n",
       "        [ 3, 11,  2, 10, 12,  7, 14,  1],\n",
       "        [12,  9,  3,  7,  6,  9, 13,  4],\n",
       "        [ 3, 14,  3,  5, 10, 11, 15, 13],\n",
       "        [ 6,  9, 12, 15,  7, 14,  4,  2],\n",
       "        [10,  9, 14, 14,  5,  9,  7,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  4,  8],\n",
       "        [ 6,  3, 12,  8,  4,  7, 10, 13],\n",
       "        [14,  7,  7,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  9,  2, 14, 14,  5],\n",
       "        [10,  4,  7, 15, 11,  8,  6,  8],\n",
       "        [ 7,  1,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 12,  7, 10,  5, 13, 13],\n",
       "        [ 3,  5,  7, 13,  8, 13,  7,  8],\n",
       "        [ 0,  0,  0,  0,  7,  3, 14,  9],\n",
       "        [ 5, 15,  7, 12,  9,  6, 10, 13],\n",
       "        [15, 11,  5,  4, 14,  7,  4,  8],\n",
       "        [11, 15,  5, 12,  7,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 15,  4],\n",
       "        [14,  4,  2,  3, 11,  3,  4,  5],\n",
       "        [ 8,  9, 11,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  7,  2,  9,  4],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  4],\n",
       "        [ 7, 13, 11, 13,  2,  4, 13,  6],\n",
       "        [14, 11,  5,  9, 13, 15,  3, 14],\n",
       "        [14, 15,  1,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  1,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  6, 11],\n",
       "        [11,  7,  4, 10,  1,  0,  0,  0],\n",
       "        [ 0,  0,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 12, 15, 15, 15,  9],\n",
       "        [ 8,  2, 13,  6,  2,  6, 10,  7],\n",
       "        [ 7, 13,  9, 10,  2,  7, 15,  9],\n",
       "        [10, 14,  4,  8,  7, 13, 14, 15],\n",
       "        [10, 15,  5,  9,  7,  8,  8,  4],\n",
       "        [ 2, 14, 11, 15, 13, 12,  1,  0],\n",
       "        [ 0,  0,  0,  4, 10,  8, 12,  6],\n",
       "        [ 3, 15, 11, 13,  2, 15, 12,  8],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0,  6, 11, 13,  6],\n",
       "        [ 4,  3,  6, 10, 13, 11, 12, 12],\n",
       "        [ 6,  9,  5,  9, 13, 11,  9, 15],\n",
       "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 12, 13, 10,  2,  5,  4],\n",
       "        [13,  1,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  1,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 13, 13],\n",
       "        [ 3,  3, 10,  7, 15,  7, 13, 13],\n",
       "        [ 2, 13, 10, 14, 14, 11, 13, 10],\n",
       "        [ 2, 15, 10,  9,  4, 11, 11,  7],\n",
       "        [ 7,  1,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 12,  8, 11],\n",
       "        [ 8,  6,  3,  8,  2, 11,  2,  8],\n",
       "        [ 9,  1,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:, 1:].reshape(64, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6ffe1d-1863-4ee7-aac6-f3b22504a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2\n",
    "encoder_global_attn = False\n",
    "doc = torch.tensor([1, 1, 2, 2, 2, 3]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5b1077-711a-4f84-ba08-958b5af962cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    if encoder_global_attn:\n",
    "        causal_mask = q_b_idx >= kv_b_idx\n",
    "        doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "        return causal_mask & doc_mask\n",
    "    else:\n",
    "        block_mask = q_b_idx == kv_b_idx\n",
    "        return block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08d300cf-ecda-4124-9eb1-30c78a1931d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result[i, j] = mask_mod(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddfd2f1-49df-479f-802c-35e037af5032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25b4794b-c437-41d9-abbb-b237589700ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 4\n",
    "n = 2\n",
    "encoder_global_attn = False\n",
    "old = True\n",
    "new = True\n",
    "doc = torch.tensor([1, 1, 2]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c05380ae-b05b-4621-b8d6-455c46329e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    block_mask = q_b_idx == kv_b_idx\n",
    "    if encoder_global_attn:\n",
    "        q_r_idx = q_idx % e\n",
    "        kv_r_idx = kv_idx % e\n",
    "        q_old = q_r_idx < n\n",
    "        q_new = q_r_idx >= n\n",
    "        kv_old = kv_r_idx < n\n",
    "        kv_cond = True if new else kv_old\n",
    "        causal_mask = q_b_idx >= kv_b_idx\n",
    "        doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "        return (\n",
    "            q_old & kv_old & causal_mask & doc_mask\n",
    "            | q_new & kv_cond & block_mask\n",
    "        )\n",
    "    else:\n",
    "        if old and new:\n",
    "            return block_mask\n",
    "        if not old and not new:\n",
    "            kv_old = (kv_idx % e) < n\n",
    "            return block_mask & kv_old\n",
    "        if old and not new:\n",
    "            q_new = (q_idx % e) >= n\n",
    "            kv_new = (kv_idx % e) >= n\n",
    "            return block_mask & ~(q_new & kv_new)\n",
    "        if not old and new:\n",
    "            q_old = (q_idx % e) < n\n",
    "            kv_new = (kv_idx % e) >= n\n",
    "            return block_mask & ~(q_old & kv_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04fac30c-8e0a-43ee-8207-5d0ddefa775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result[i, j] = mask_mod(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b23f0045-60f8-4513-ab8c-18c9704a29ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61f8f990-8e59-487c-ba24-ca616bcfd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "n = 2\n",
    "e = k + n\n",
    "include_previous_block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8d71fc0-be98-4994-b88d-6842c225da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = torch.tensor([1, 1, 2]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ed268271-0336-41cf-b89f-ddd51ba6c7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd14812b-7ff0-40a2-ae61-1021ada3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod_2(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    q_r_idx = q_idx % e\n",
    "    kv_r_idx = kv_idx % e\n",
    "    q_token = q_r_idx >= k\n",
    "    kv_token = kv_r_idx >= k\n",
    "    q_latent = q_r_idx < k\n",
    "    kv_latent = kv_r_idx < k\n",
    "    same_block = q_b_idx == kv_b_idx\n",
    "    causal_mask = q_idx >= kv_idx\n",
    "    intra_block_token_mask = q_token & same_block & causal_mask\n",
    "    inter_block_token_mask = False\n",
    "    if include_previous_block:\n",
    "        previous_block = q_b_idx == kv_b_idx + 1\n",
    "        doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "        inter_block_token_mask = q_token & kv_token & previous_block & doc_mask\n",
    "    intra_block_latent_mask = q_latent & kv_latent & same_block\n",
    "    return (\n",
    "        intra_block_token_mask\n",
    "        | inter_block_token_mask\n",
    "        | intra_block_latent_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "71f89adc-3b9a-485f-ba43-2a1e14d23429",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result[i, j] = mask_mod_2(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "25674d0f-e342-4dfd-9c28-36345b9f25a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "897c83ab-946e-4f47-ada0-52fc5fcfa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "n = 3\n",
    "w = 3\n",
    "e = m + n\n",
    "doc = torch.tensor([1, 1]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "038680e0-5c37-4f5b-8580-cdeaf810a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    q_r_idx = q_idx % e\n",
    "    kv_r_idx = kv_idx % e\n",
    "    q_new = q_r_idx >= m\n",
    "    kv_new = kv_r_idx >= m\n",
    "    q_old = q_r_idx < m\n",
    "    kv_old = kv_r_idx < m\n",
    "    same_block = q_b_idx == kv_b_idx\n",
    "    causal_mask = q_idx >= kv_idx\n",
    "    kv_b_idx_m = max(kv_b_idx - 1, 0)\n",
    "    doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "    old_doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx_m]\n",
    "    window_mask = q_r_idx + q_b_idx * n < kv_r_idx + kv_b_idx * n + w\n",
    "\n",
    "    new_new_mask = q_new & kv_new & causal_mask & window_mask & doc_mask\n",
    "    any_old_mask = (q_new & old_doc_mask | q_old) & kv_old & same_block\n",
    "    return new_new_mask | any_old_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0757af1-b5b1-40b7-88ea-eeaf6df83844",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result[i, j] = mask_mod(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d831dd89-e394-43b3-9b1d-91203504c6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b997a60c-2be6-4867-9a8b-91301ac406eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    q_r_idx = q_idx % e\n",
    "    kv_r_idx = kv_idx % e\n",
    "    q_token = q_r_idx >= k\n",
    "    kv_token = kv_r_idx >= k\n",
    "    q_latent = q_r_idx < k\n",
    "    kv_latent = kv_r_idx < k\n",
    "    same_block = q_b_idx == kv_b_idx\n",
    "    causal_mask = q_idx >= kv_idx\n",
    "    kv_b_idx_m = max(kv_b_idx - 1, 0)\n",
    "    doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "    latent_doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx_m]\n",
    "    window_mask = q_b_idx < kv_b_idx + a\n",
    "\n",
    "    token_token_mask = q_token & kv_token & causal_mask & window_mask & doc_mask\n",
    "    token_latent_mask = q_token & kv_latent & same_block & latent_doc_mask\n",
    "    latent_latent_mask = q_latent & kv_latent & same_block\n",
    "    return (\n",
    "        token_token_mask\n",
    "        | token_latent_mask\n",
    "        | latent_latent_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e6e34d9-b129-48a4-9e5f-98132513e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "n = 2\n",
    "e = k + n\n",
    "a = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e137f7a9-3a98-4d60-b7c5-fcca790d88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = torch.tensor([1, 1, 2]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7266a7de-d409-4935-9e3b-b17adf89aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result[i, j] = mask_mod(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d7a950a-269a-4f1d-af23-5c0c6848d251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9e5aa71-cf7b-4b60-a18f-94ec99afe70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mod_3(b, h, q_idx, kv_idx):\n",
    "    q_b_idx = q_idx // e\n",
    "    kv_b_idx = kv_idx // e\n",
    "    q_r_idx = q_idx % e\n",
    "    kv_r_idx = kv_idx % e\n",
    "    block_mask = q_b_idx == kv_b_idx\n",
    "    latent_mask = q_r_idx == kv_r_idx\n",
    "    causal_mask = q_b_idx >= kv_b_idx\n",
    "    doc_mask = doc[b, q_b_idx] == doc[b, kv_b_idx]\n",
    "    return block_mask | (latent_mask & causal_mask & doc_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a43bba2-0f02-460b-8b8b-fb35c019a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "844ac54c-c4da-4682-8844-ece8f5025792",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = torch.tensor([1, 1, 1, 2, 2, 3]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57790eca-78df-4cb8-aea1-d75521414821",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = torch.full((12, 12), 2)\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        result_3[i, j] = mask_mod_3(0, 0, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99c97fa6-c524-4f65-b5f2-5232e3806340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
